{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data loading and preprcessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('News_train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Arabic stop words\n",
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove English characters\n",
    "    text = re.sub(r'[A-Za-z]', '', text)\n",
    "    # Remove \"ال\"\n",
    "    text = re.sub(r'\\bال', '', text)\n",
    "    # Remove Arabic diacritical marks (الحركات)\n",
    "    diacritics = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(diacritics, '', text)\n",
    "    # Remove punctuation and replace with space\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert همزات / تاء مربوطة\n",
    "    tokens = [re.sub(\"[إأٱآا]\", \"ا\", token) for token in tokens]\n",
    "    tokens = [re.sub(\"ؤ\", \"ء\", token) for token in tokens]\n",
    "    tokens = [re.sub(\"ئ\", \"ء\", token) for token in tokens]\n",
    "    tokens = [re.sub(\"ة\", \"ه\", token) for token in tokens]\n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in arabic_stopwords]\n",
    "    # Join tokens back to text\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    # Remove extra spaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['News'] = df['News'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df['News']\n",
    "y = df['Type']\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_val.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TF-IDF, One-hot + Naive Base, Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=300)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_train_tfidf_df = pd.DataFrame(X_train_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "X_val_tfidf_df = pd.DataFrame(X_val_tfidf.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF with Naive Base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.68      0.51       176\n",
      "           1       0.97      0.52      0.68       647\n",
      "           2       0.61      0.82      0.70       163\n",
      "           3       0.07      0.79      0.14        14\n",
      "\n",
      "    accuracy                           0.60      1000\n",
      "   macro avg       0.52      0.70      0.51      1000\n",
      "weighted avg       0.80      0.60      0.65      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "NB_classifier = GaussianNB()\n",
    "NB_classifier.fit(X_train_tfidf_df, y_train)\n",
    "\n",
    "y_pred = NB_classifier.predict(X_val_tfidf_df)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF with Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Logistic Regression): 0.857\n",
      "Classification Report (Logistic Regression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.60      0.70       176\n",
      "           1       0.85      0.96      0.90       647\n",
      "           2       0.90      0.81      0.85       163\n",
      "           3       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.65      0.59      0.61      1000\n",
      "weighted avg       0.85      0.86      0.85      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SaifD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SaifD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SaifD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR_classifier = LogisticRegression(max_iter=1000)\n",
    "LR_classifier.fit(X_train_tfidf_df, y_train)\n",
    "\n",
    "y_pred_lr = LR_classifier.predict(X_val_tfidf_df)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_lr = accuracy_score(y_val, y_pred_lr)\n",
    "print(f'Accuracy (Logistic Regression): {accuracy_lr}')\n",
    "print('Classification Report (Logistic Regression):')\n",
    "print(classification_report(y_val, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "one_vec = CountVectorizer(binary=True, max_features=300)\n",
    "\n",
    "X_train_one_hot_matrix  = one_vec.fit_transform(X_train)\n",
    "X_train_one_hot_df = pd.DataFrame(X_train_one_hot_matrix.toarray(), columns=one_vec.get_feature_names_out())\n",
    "\n",
    "X_val_one_hot_matrix  = one_vec.transform(X_val)\n",
    "X_val_one_hot_df = pd.DataFrame(X_val_one_hot_matrix.toarray(), columns=one_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot wiht Naive Base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.57\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.64      0.48       176\n",
      "           1       0.97      0.48      0.65       647\n",
      "           2       0.57      0.82      0.67       163\n",
      "           3       0.07      0.79      0.13        14\n",
      "\n",
      "    accuracy                           0.57      1000\n",
      "   macro avg       0.50      0.68      0.48      1000\n",
      "weighted avg       0.79      0.57      0.61      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_classifier = GaussianNB()\n",
    "NB_classifier.fit(X_train_one_hot_df, y_train)\n",
    "\n",
    "y_pred = NB_classifier.predict(X_val_one_hot_df)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot wiht Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Logistic Regression): 0.859\n",
      "Classification Report (Logistic Regression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.62      0.71       176\n",
      "           1       0.85      0.95      0.90       647\n",
      "           2       0.92      0.82      0.87       163\n",
      "           3       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.65      0.60      0.62      1000\n",
      "weighted avg       0.85      0.86      0.85      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_classifier = LogisticRegression(max_iter=1000)\n",
    "LR_classifier.fit(X_train_one_hot_df, y_train)\n",
    "\n",
    "y_pred_lr = LR_classifier.predict(X_val_one_hot_df)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_lr = accuracy_score(y_val, y_pred_lr)\n",
    "print(f'Accuracy (Logistic Regression): {accuracy_lr}')\n",
    "print('Classification Report (Logistic Regression):')\n",
    "print(classification_report(y_val, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word2Vec [CBow, Skip Gram] + Naive Base, Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "w2v_model_CBoW = gensim.models.Word2Vec.load('C:/Users/SaifD/Desktop/NLP-Final/wiki_cbow_300/wikipedia_cbow_300')\n",
    "\n",
    "OOV_tokens_CBoW = []\n",
    "train_tokens_CBoW = []\n",
    "val_tokens_CBoW = []\n",
    "\n",
    "def get_doc_vec_CBoW(sent, model, data_type):\n",
    "    w2v_embeddings = []\n",
    "    tokens = sent.split()\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            if data_type == 'train':\n",
    "                w2v_embeddings.append(model.wv[word])\n",
    "                train_tokens_CBoW.append(word)\n",
    "            else:\n",
    "                w2v_embeddings.append(model.wv[word])\n",
    "                val_tokens_CBoW.append(word)\n",
    "        except KeyError:\n",
    "            OOV_tokens_CBoW.append(word)\n",
    "            continue\n",
    "    if len(w2v_embeddings) == 0:\n",
    "        return None\n",
    "    return np.mean(w2v_embeddings, axis=0)\n",
    "\n",
    "# Generate embeddings for training and validation sets\n",
    "X_train_w2v_embeddings_CBoW = X_train.apply(lambda sent: get_doc_vec_CBoW(sent, w2v_model_CBoW, 'train'))\n",
    "X_val_w2v_embeddings_CBoW = X_val.apply(lambda sent: get_doc_vec_CBoW(sent, w2v_model_CBoW, 'test'))\n",
    "\n",
    "# Initialize lists to store embeddings\n",
    "X_train_w2v_embeddings_list_CBoW = []\n",
    "X_val_w2v_embeddings_list_CBoW = []\n",
    "\n",
    "# Convert embeddings from the pandas Series to lists and handle None values\n",
    "zero_vector = np.zeros(w2v_model_CBoW.vector_size)\n",
    "\n",
    "for embedding in X_train_w2v_embeddings_CBoW:\n",
    "    if embedding is not None:\n",
    "        X_train_w2v_embeddings_list_CBoW.append(embedding)\n",
    "    else:\n",
    "        X_train_w2v_embeddings_list_CBoW.append(zero_vector)\n",
    "\n",
    "for embedding in X_val_w2v_embeddings_CBoW:\n",
    "    if embedding is not None:\n",
    "        X_val_w2v_embeddings_list_CBoW.append(embedding)\n",
    "    else:\n",
    "        X_val_w2v_embeddings_list_CBoW.append(zero_vector)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train_w2v_embeddings_array_CBoW = np.array(X_train_w2v_embeddings_list_CBoW)\n",
    "X_val_w2v_embeddings_array_CBoW = np.array(X_val_w2v_embeddings_list_CBoW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec CBoW with Naive Base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.69       176\n",
      "           1       0.92      0.86      0.89       647\n",
      "           2       0.95      0.90      0.92       163\n",
      "           3       0.16      0.64      0.25        14\n",
      "\n",
      "    accuracy                           0.84      1000\n",
      "   macro avg       0.68      0.78      0.69      1000\n",
      "weighted avg       0.87      0.84      0.85      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_classifier = GaussianNB()\n",
    "NB_classifier.fit(X_train_w2v_embeddings_array_CBoW, y_train)\n",
    "\n",
    "y_pred = NB_classifier.predict(X_val_w2v_embeddings_array_CBoW)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec CBoW with Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Logistic Regression): 0.874\n",
      "Classification Report (Logistic Regression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.68      0.72       176\n",
      "           1       0.90      0.93      0.91       647\n",
      "           2       0.90      0.92      0.91       163\n",
      "           3       0.36      0.29      0.32        14\n",
      "\n",
      "    accuracy                           0.87      1000\n",
      "   macro avg       0.73      0.70      0.72      1000\n",
      "weighted avg       0.87      0.87      0.87      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_classifier = LogisticRegression(max_iter=1000)\n",
    "LR_classifier.fit(X_train_w2v_embeddings_array_CBoW, y_train)\n",
    "\n",
    "y_pred_lr = LR_classifier.predict(X_val_w2v_embeddings_array_CBoW)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_lr = accuracy_score(y_val, y_pred_lr)\n",
    "print(f'Accuracy (Logistic Regression): {accuracy_lr}')\n",
    "print('Classification Report (Logistic Regression):')\n",
    "print(classification_report(y_val, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec with Skip Gram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained Word2Vec Skip-Gram model\n",
    "w2v_model_sg = gensim.models.Word2Vec.load('C:/Users/SaifD/Desktop/NLP-Final/wiki_sg_300/wikipedia_sg_300')\n",
    "\n",
    "# Initialize lists for storing tokens and embeddings\n",
    "OOV_tokens_sg = []\n",
    "train_tokens_sg = []\n",
    "val_tokens_sg = []\n",
    "\n",
    "def get_doc_vec_sg(sent, model, data_type):\n",
    "    w2v_embeddings = []\n",
    "    tokens = sent.split()\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            if data_type == 'train':\n",
    "                w2v_embeddings.append(model.wv[word])\n",
    "                train_tokens_sg.append(word)\n",
    "            else:\n",
    "                w2v_embeddings.append(model.wv[word])\n",
    "                val_tokens_sg.append(word)\n",
    "        except KeyError:\n",
    "            OOV_tokens_sg.append(word)\n",
    "            continue\n",
    "    if len(w2v_embeddings) == 0:\n",
    "        return None\n",
    "    return np.mean(w2v_embeddings, axis=0)\n",
    "\n",
    "# Generate embeddings for training and validation sets\n",
    "X_train_w2v_embeddings_sg = X_train.apply(lambda sent: get_doc_vec_sg(sent, w2v_model_sg, 'train'))\n",
    "X_val_w2v_embeddings_sg = X_val.apply(lambda sent: get_doc_vec_sg(sent, w2v_model_sg, 'test'))\n",
    "\n",
    "# Initialize lists to store embeddings\n",
    "X_train_w2v_embeddings_list_sg = []\n",
    "X_val_w2v_embeddings_list_sg = []\n",
    "\n",
    "# Convert embeddings from the pandas Series to lists and handle None values\n",
    "zero_vector = np.zeros(w2v_model_sg.vector_size)\n",
    "\n",
    "for embedding in X_train_w2v_embeddings_sg:\n",
    "    if embedding is not None:\n",
    "        X_train_w2v_embeddings_list_sg.append(embedding)\n",
    "    else:\n",
    "        X_train_w2v_embeddings_list_sg.append(zero_vector)\n",
    "\n",
    "for embedding in X_val_w2v_embeddings_sg:\n",
    "    if embedding is not None:\n",
    "        X_val_w2v_embeddings_list_sg.append(embedding)\n",
    "    else:\n",
    "        X_val_w2v_embeddings_list_sg.append(zero_vector)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train_w2v_embeddings_array_sg = np.array(X_train_w2v_embeddings_list_sg)\n",
    "X_val_w2v_embeddings_array_sg = np.array(X_val_w2v_embeddings_list_sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec with Skip Gram with Naive Base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75       176\n",
      "           1       0.93      0.90      0.91       647\n",
      "           2       0.97      0.91      0.94       163\n",
      "           3       0.31      0.64      0.42        14\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.73      0.81      0.75      1000\n",
      "weighted avg       0.89      0.88      0.88      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_classifier = GaussianNB()\n",
    "NB_classifier.fit(X_train_w2v_embeddings_array_sg, y_train)\n",
    "\n",
    "y_pred = NB_classifier.predict(X_val_w2v_embeddings_array_sg)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec with Skip Gram with Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Logistic Regression): 0.917\n",
      "Classification Report (Logistic Regression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.76      0.81       176\n",
      "           1       0.93      0.96      0.94       647\n",
      "           2       0.95      0.96      0.95       163\n",
      "           3       0.62      0.36      0.45        14\n",
      "\n",
      "    accuracy                           0.92      1000\n",
      "   macro avg       0.84      0.76      0.79      1000\n",
      "weighted avg       0.91      0.92      0.91      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_classifier = LogisticRegression(max_iter=1000)\n",
    "LR_classifier.fit(X_train_w2v_embeddings_array_sg, y_train)\n",
    "\n",
    "y_pred_lr = LR_classifier.predict(X_val_w2v_embeddings_array_sg)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_lr = accuracy_score(y_val, y_pred_lr)\n",
    "print(f'Accuracy (Logistic Regression): {accuracy_lr}')\n",
    "print('Classification Report (Logistic Regression):')\n",
    "print(classification_report(y_val, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BERT Word embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SaifD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"aubmindlab/bert-base-arabert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Get the embeddings of the [CLS] token\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :] # [batch_size, sequence_length, hidden_size]\n",
    "    return cls_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BERT embeddings for train and val data\n",
    "X_train_BERT_embeddings = np.vstack(X_train.apply(lambda x: get_bert_embeddings(x)).values)\n",
    "X_val_BERT_embeddings = np.vstack(X_val.apply(lambda x: get_bert_embeddings(x)).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT with Naive Base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.73      0.66       176\n",
      "           1       0.91      0.85      0.88       647\n",
      "           2       0.97      0.87      0.92       163\n",
      "           3       0.34      0.79      0.48        14\n",
      "\n",
      "    accuracy                           0.83      1000\n",
      "   macro avg       0.71      0.81      0.73      1000\n",
      "weighted avg       0.86      0.83      0.84      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_classifier = GaussianNB()\n",
    "NB_classifier.fit(X_train_BERT_embeddings, y_train)\n",
    "\n",
    "y_pred = NB_classifier.predict(X_val_BERT_embeddings)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT with Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Logistic Regression): 0.918\n",
      "Classification Report (Logistic Regression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83       176\n",
      "           1       0.94      0.95      0.94       647\n",
      "           2       0.95      0.96      0.95       163\n",
      "           3       0.55      0.43      0.48        14\n",
      "\n",
      "    accuracy                           0.92      1000\n",
      "   macro avg       0.82      0.79      0.80      1000\n",
      "weighted avg       0.92      0.92      0.92      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_classifier = LogisticRegression(max_iter=1000)\n",
    "LR_classifier.fit(X_train_BERT_embeddings, y_train)\n",
    "\n",
    "y_pred_lr = LR_classifier.predict(X_val_BERT_embeddings)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_lr = accuracy_score(y_val, y_pred_lr)\n",
    "print(f'Accuracy (Logistic Regression): {accuracy_lr}')\n",
    "print('Classification Report (Logistic Regression):')\n",
    "print(classification_report(y_val, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4000  \n",
    "tokenizer = Tokenizer(num_words=vocab_size) \n",
    "tokenizer.fit_on_texts(df['News'])\n",
    "seq = tokenizer.texts_to_sequences(df['News'])\n",
    "\n",
    "max_length = 100  \n",
    "pad_seq = pad_sequences(seq, maxlen=max_length, padding='pre')\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_labels = label_encoder.fit_transform(df['Type'])\n",
    "y_labels = tf.keras.utils.to_categorical(y_labels)  \n",
    "\n",
    "\n",
    "pad_seq_X_train, pad_seq_X_test, y_train, y_test = train_test_split(pad_seq, y_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM + TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SaifD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.5907 - loss: 1.0822 - val_accuracy: 0.6620 - val_loss: 0.8252\n",
      "Epoch 2/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.6831 - loss: 0.8069 - val_accuracy: 0.7440 - val_loss: 0.7035\n",
      "Epoch 3/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7588 - loss: 0.6502 - val_accuracy: 0.7610 - val_loss: 0.6433\n",
      "Epoch 4/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7981 - loss: 0.5563 - val_accuracy: 0.7800 - val_loss: 0.6252\n",
      "Epoch 5/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8072 - loss: 0.5135 - val_accuracy: 0.7770 - val_loss: 0.6309\n",
      "Epoch 6/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8272 - loss: 0.4897 - val_accuracy: 0.7820 - val_loss: 0.6033\n",
      "Epoch 7/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8440 - loss: 0.4195 - val_accuracy: 0.7700 - val_loss: 0.6529\n",
      "Epoch 8/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8499 - loss: 0.3987 - val_accuracy: 0.7830 - val_loss: 0.6142\n",
      "Epoch 9/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8555 - loss: 0.3739 - val_accuracy: 0.7760 - val_loss: 0.6604\n",
      "Epoch 10/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8732 - loss: 0.3406 - val_accuracy: 0.7790 - val_loss: 0.6755\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7912 - loss: 0.6758\n",
      "Validation Accuracy: 77.90%\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    economic       0.72      0.53      0.61       176\n",
      "    politics       0.82      0.89      0.85       647\n",
      "       sport       0.70      0.67      0.69       163\n",
      "        tech       0.15      0.14      0.15        14\n",
      "\n",
      "    accuracy                           0.78      1000\n",
      "   macro avg       0.60      0.56      0.57      1000\n",
      "weighted avg       0.77      0.78      0.77      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=300, input_length=max_length, weights=[X_train_tfidf_df], trainable=False))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=False)))\n",
    "model.add(Dense(4, activation='softmax'))  \n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(pad_seq_X_train, y_train, epochs=10, verbose=1, validation_data=(pad_seq_X_test, y_test))\n",
    "print('-------------------------------------------------------------------------------------------------------------')\n",
    "val_loss, val_accuracy = model.evaluate(pad_seq_X_test, y_test)\n",
    "print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "y_pred = model.predict(pad_seq_X_test)\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "report = classification_report(y_true_labels, y_pred_labels, target_names=label_encoder.classes_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM + One-hot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SaifD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.6052 - loss: 1.0317 - val_accuracy: 0.7220 - val_loss: 0.7298\n",
      "Epoch 2/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7351 - loss: 0.6921 - val_accuracy: 0.7700 - val_loss: 0.6257\n",
      "Epoch 3/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.7890 - loss: 0.5702 - val_accuracy: 0.7770 - val_loss: 0.6080\n",
      "Epoch 4/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8168 - loss: 0.4996 - val_accuracy: 0.7890 - val_loss: 0.5907\n",
      "Epoch 5/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8336 - loss: 0.4505 - val_accuracy: 0.7850 - val_loss: 0.5936\n",
      "Epoch 6/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8526 - loss: 0.3946 - val_accuracy: 0.7910 - val_loss: 0.6379\n",
      "Epoch 7/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8701 - loss: 0.3654 - val_accuracy: 0.8040 - val_loss: 0.6365\n",
      "Epoch 8/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8746 - loss: 0.3315 - val_accuracy: 0.7930 - val_loss: 0.6617\n",
      "Epoch 9/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8974 - loss: 0.2845 - val_accuracy: 0.7920 - val_loss: 0.6572\n",
      "Epoch 10/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9078 - loss: 0.2580 - val_accuracy: 0.7670 - val_loss: 0.7164\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7650 - loss: 0.7091\n",
      "Validation Accuracy: 76.70%\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    economic       0.62      0.59      0.60       176\n",
      "    politics       0.84      0.84      0.84       647\n",
      "       sport       0.65      0.74      0.69       163\n",
      "        tech       0.67      0.14      0.24        14\n",
      "\n",
      "    accuracy                           0.77      1000\n",
      "   macro avg       0.69      0.58      0.59      1000\n",
      "weighted avg       0.77      0.77      0.76      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=300, input_length=max_length, weights=[X_train_one_hot_df], trainable=False))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=False)))\n",
    "model.add(Dense(4, activation='softmax'))  \n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(pad_seq_X_train, y_train, epochs=10, verbose=1, validation_data=(pad_seq_X_test, y_test))\n",
    "print('-------------------------------------------------------------------------------------------------------------')\n",
    "val_loss, val_accuracy = model.evaluate(pad_seq_X_test, y_test)\n",
    "print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "y_pred = model.predict(pad_seq_X_test)\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "report = classification_report(y_true_labels, y_pred_labels, target_names=label_encoder.classes_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM + word2Vec CBoW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_nparray = np.concatenate(X_train_w2v_embeddings_CBoW)\n",
    "X_train_w2v_embeddings_CBoW_nparray = convert_to_nparray.reshape(4000,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SaifD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.6131 - loss: 1.0101 - val_accuracy: 0.6900 - val_loss: 0.7918\n",
      "Epoch 2/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7406 - loss: 0.6944 - val_accuracy: 0.7740 - val_loss: 0.6111\n",
      "Epoch 3/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8063 - loss: 0.5228 - val_accuracy: 0.8140 - val_loss: 0.5293\n",
      "Epoch 4/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8673 - loss: 0.3705 - val_accuracy: 0.8370 - val_loss: 0.4797\n",
      "Epoch 5/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8980 - loss: 0.2929 - val_accuracy: 0.8450 - val_loss: 0.4996\n",
      "Epoch 6/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9302 - loss: 0.1991 - val_accuracy: 0.8550 - val_loss: 0.4608\n",
      "Epoch 7/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9699 - loss: 0.1174 - val_accuracy: 0.8360 - val_loss: 0.5992\n",
      "Epoch 8/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9697 - loss: 0.1002 - val_accuracy: 0.8380 - val_loss: 0.5779\n",
      "Epoch 9/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9902 - loss: 0.0626 - val_accuracy: 0.8440 - val_loss: 0.5844\n",
      "Epoch 10/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9939 - loss: 0.0328 - val_accuracy: 0.7890 - val_loss: 0.8176\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7872 - loss: 0.8845\n",
      "Validation Accuracy: 78.90%\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    economic       0.58      0.76      0.66       176\n",
      "    politics       0.92      0.79      0.85       647\n",
      "       sport       0.68      0.87      0.77       163\n",
      "        tech       0.17      0.07      0.10        14\n",
      "\n",
      "    accuracy                           0.79      1000\n",
      "   macro avg       0.59      0.62      0.59      1000\n",
      "weighted avg       0.81      0.79      0.79      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=300, input_length=max_length, weights=[X_train_w2v_embeddings_CBoW_nparray], trainable=False))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=False)))\n",
    "model.add(Dense(4, activation='softmax'))  \n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(pad_seq_X_train, y_train, epochs=10, verbose=1, validation_data=(pad_seq_X_test, y_test))\n",
    "print('-------------------------------------------------------------------------------------------------------------')\n",
    "val_loss, val_accuracy = model.evaluate(pad_seq_X_test, y_test)\n",
    "print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "y_pred = model.predict(pad_seq_X_test)\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "report = classification_report(y_true_labels, y_pred_labels, target_names=label_encoder.classes_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM + word2Vec Skip Gram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_nparray = np.concatenate(X_train_w2v_embeddings_sg)\n",
    "X_train_w2v_embeddings_sg_nparray = convert_to_nparray.reshape(4000,300)\n",
    "# X_train_w2v_embeddings_sg_nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SaifD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.6000 - loss: 1.0376 - val_accuracy: 0.6760 - val_loss: 0.9152\n",
      "Epoch 2/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.6541 - loss: 0.8948 - val_accuracy: 0.6910 - val_loss: 0.7774\n",
      "Epoch 3/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7001 - loss: 0.7900 - val_accuracy: 0.7380 - val_loss: 0.7130\n",
      "Epoch 4/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7233 - loss: 0.7110 - val_accuracy: 0.7640 - val_loss: 0.6499\n",
      "Epoch 5/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.7606 - loss: 0.6389 - val_accuracy: 0.7070 - val_loss: 0.7645\n",
      "Epoch 6/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7498 - loss: 0.6458 - val_accuracy: 0.7710 - val_loss: 0.6335\n",
      "Epoch 7/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.7943 - loss: 0.5716 - val_accuracy: 0.7540 - val_loss: 0.6434\n",
      "Epoch 8/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8240 - loss: 0.4752 - val_accuracy: 0.7950 - val_loss: 0.6119\n",
      "Epoch 9/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8408 - loss: 0.4236 - val_accuracy: 0.8100 - val_loss: 0.5567\n",
      "Epoch 10/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.8630 - loss: 0.3864 - val_accuracy: 0.7890 - val_loss: 0.5628\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7897 - loss: 0.5706\n",
      "Validation Accuracy: 78.90%\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    economic       0.71      0.60      0.65       176\n",
      "    politics       0.83      0.90      0.86       647\n",
      "       sport       0.76      0.60      0.67       163\n",
      "        tech       0.06      0.07      0.06        14\n",
      "\n",
      "    accuracy                           0.79      1000\n",
      "   macro avg       0.59      0.54      0.56      1000\n",
      "weighted avg       0.79      0.79      0.78      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=300, input_length=max_length, weights=[X_train_w2v_embeddings_sg_nparray], trainable=False))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=False)))\n",
    "model.add(Dense(4, activation='softmax'))  \n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(pad_seq_X_train, y_train, epochs=10, verbose=1, validation_data=(pad_seq_X_test, y_test))\n",
    "print('-------------------------------------------------------------------------------------------------------------')\n",
    "val_loss, val_accuracy = model.evaluate(pad_seq_X_test, y_test)\n",
    "print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "y_pred = model.predict(pad_seq_X_test)\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "report = classification_report(y_true_labels, y_pred_labels, target_names=label_encoder.classes_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM + BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SaifD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 0.6073 - loss: 1.0645 - val_accuracy: 0.6450 - val_loss: 0.9031\n",
      "Epoch 2/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.6306 - loss: 0.9182 - val_accuracy: 0.7090 - val_loss: 0.7948\n",
      "Epoch 3/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.7063 - loss: 0.7657 - val_accuracy: 0.7450 - val_loss: 0.6333\n",
      "Epoch 4/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.7789 - loss: 0.5992 - val_accuracy: 0.7730 - val_loss: 0.5772\n",
      "Epoch 5/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.8301 - loss: 0.4827 - val_accuracy: 0.8180 - val_loss: 0.4924\n",
      "Epoch 6/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.8491 - loss: 0.4037 - val_accuracy: 0.8110 - val_loss: 0.4992\n",
      "Epoch 7/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.8623 - loss: 0.3679 - val_accuracy: 0.8300 - val_loss: 0.4674\n",
      "Epoch 8/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.8957 - loss: 0.2846 - val_accuracy: 0.8350 - val_loss: 0.4810\n",
      "Epoch 9/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.9257 - loss: 0.2234 - val_accuracy: 0.8170 - val_loss: 0.4934\n",
      "Epoch 10/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9415 - loss: 0.1870 - val_accuracy: 0.8550 - val_loss: 0.4468\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8561 - loss: 0.4403\n",
      "Validation Accuracy: 85.50%\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    economic       0.77      0.69      0.73       176\n",
      "    politics       0.87      0.94      0.90       647\n",
      "       sport       0.92      0.75      0.83       163\n",
      "        tech       0.40      0.14      0.21        14\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.74      0.63      0.67      1000\n",
      "weighted avg       0.85      0.85      0.85      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=768, input_length=max_length, weights=[X_train_BERT_embeddings], trainable=False))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=False)))\n",
    "model.add(Dense(4, activation='softmax'))  \n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(pad_seq_X_train, y_train, epochs=10, verbose=1, validation_data=(pad_seq_X_test, y_test))\n",
    "print('-------------------------------------------------------------------------------------------------------------')\n",
    "val_loss, val_accuracy = model.evaluate(pad_seq_X_test, y_test)\n",
    "print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "y_pred = model.predict(pad_seq_X_test)\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "report = classification_report(y_true_labels, y_pred_labels, target_names=label_encoder.classes_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BERT Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SaifD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\SaifD\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Training Loss: 0.19\n",
      "Epoch 1/3, Accuracy: 0.93\n",
      "Epoch 2/3, Training Loss: 0.08\n",
      "Epoch 2/3, Accuracy: 0.94\n",
      "Epoch 3/3, Training Loss: 0.05\n",
      "Epoch 3/3, Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "\n",
    " \n",
    "model_name = 'aubmindlab/bert-base-arabert'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4) # specifically designed for sequence classification tasks\n",
    "# It includes a classification head on top of the pre-trained BERT model.\n",
    "\n",
    "def tokenize(sentences, tokenizer, max_len):\n",
    "    tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_len)\n",
    "    return tokens['input_ids'], tokens['attention_mask']\n",
    " \n",
    "train_sentences = X_train.tolist()\n",
    "test_sentences = X_val.tolist()\n",
    "\n",
    "# Find the maximum sequence length\n",
    "max_sequence_len = max(len(x.split()) for x in train_sentences + test_sentences)\n",
    "\n",
    "# Tokenize data\n",
    "train_input_ids, train_attention_mask = tokenize(train_sentences, tokenizer, max_sequence_len)\n",
    "test_input_ids, test_attention_mask = tokenize(test_sentences, tokenizer, max_sequence_len)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, torch.tensor(y_train))\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, torch.tensor(y_val))\n",
    "\n",
    "# Create DataLoaders\n",
    "# Use it to load data in batches\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    " \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    " \n",
    "optimizer = AdamW(model.parameters(), lr=2e-5) # Adma with Weight Decay - provide better regularization\n",
    " \n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0 #-----------------\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "        # Perform forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item() #-----------------\n",
    "        loss.backward()\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss:.2f}')\n",
    " \n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            # Get predictions by finding the index of the maximum logit\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    " \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    economic       0.87      0.90      0.89       176\n",
      "    politics       0.97      0.96      0.96       647\n",
      "       sport       0.96      0.98      0.97       163\n",
      "        tech       0.83      0.71      0.77        14\n",
      "\n",
      "    accuracy                           0.95      1000\n",
      "   macro avg       0.91      0.89      0.90      1000\n",
      "weighted avg       0.95      0.95      0.95      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(model, test_loader, device, label_encoder):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "    \n",
    "    class_names = label_encoder.inverse_transform([0, 1, 2, 3])\n",
    "    report = classification_report(all_labels, all_predictions, target_names=class_names)\n",
    "    print(report)\n",
    "\n",
    "# Call the function after training\n",
    "evaluate_model(model, test_loader, device, label_encoder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
